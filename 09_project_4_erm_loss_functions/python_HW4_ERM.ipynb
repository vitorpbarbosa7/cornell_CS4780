{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Project 4: Empirical Risk Minimization</h2>\n",
    "\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"./spam.jpeg\" width=\"200px\" />\n",
    "    </center>\n",
    "      <p><cite><center>\"One person's spam is another person's dinner.\"<br>\n",
    "       -- ancient German wisdon\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>\n",
    "    In this project you will be building an email spam filter.</p>\n",
    "\n",
    "<strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.\n",
    "\n",
    "Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.\n",
    "\n",
    "<p><strong>Evaluation:</strong> Your code will be autograded for technical\n",
    "correctness and--on some assignments--speed. Please <em>do not</em> change the names of any provided functions or classes within the code, or you will wreak havoc on the autograder. Furthermore, <em>any code not surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags will not be run by the autograder</em>. However, the correctness of your implementation -- not the autograder's output -- will be the final judge of your score.  If necessary, we will review and grade assignments individually to ensure that you receive due credit for your work.\n",
    "\n",
    "<p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
    "\n",
    "<p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href=\"https://piazza.com/class/jcb1ar25kjd5vq\">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing derivatives</h3>\n",
    "\n",
    "<p>  Before you dive into the programming part of this assignment you will need to derive the gradients for several loss functions. \n",
    "    <b>Please write your calculation in a comment block within your code.</b> \n",
    "</p>\n",
    "\n",
    "<p>   Derive the gradient function for each of the following loss functions with respect to the weight vector $w$. Write down the gradient update (with stepsize $c$). <br>\n",
    "(Note that:    $\\|w\\|_2^2=w^\\top w$ and  $\\lambda$ is a  non-negative constant.)\n",
    "</p>\n",
    "\n",
    "<ol>\n",
    "    <li> Ridge Regression: ${\\cal L}(w)=\\frac{1}{n}\\sum_{i=1}^n (w^\\top x_i-y_i)^2+\\lambda \\|w\\|_2^2$ </li>\n",
    "    <li> Logistic Regression: ($y_i\\in\\{+1,-1\\}$): ${\\cal L}(w)=\\sum_{i=1}^n \\log(1+\\exp{(-y_i w^\\top x_i)})$ </li>\n",
    "    <li> Hinge loss: ($y_i\\in\\{+1,-1\\}$): ${\\cal L}(w)=\\sum_{i=1}^n \\max \\left(1-y_i(w^\\top x_i+b),0\\right)+\\lambda \\|w\\|_2^2$ </li>\n",
    "</ol>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building an email spam filter</h3>\n",
    "<p> You will now implement ridge loss and the Adagrad algorithm.\n",
    "   \n",
    "The function below loads in pre-processed email data, where emails are represented as bag-of-words vectors.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T14:29:13.197305Z",
     "start_time": "2023-06-03T14:29:13.045516Z"
    }
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy import array as ar\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "#</GRADED>\n",
    "import matplotlib\n",
    "matplotlib.use('PDF')\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T14:04:34.431822Z",
     "start_time": "2023-06-03T14:04:34.117167Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../resource/lib/public/data_train/index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m input emails.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(ys))\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xs, ys\n\u001b[0;32m---> 39\u001b[0m X,Y \u001b[38;5;241m=\u001b[39m \u001b[43mloadspamdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextractfeaturesnaive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m X\u001b[38;5;241m.\u001b[39mshape\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mloadspamdata\u001b[0;34m(extractfeatures, B, path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m     path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     27\u001b[0m     allemails \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[1;32m     29\u001b[0m xs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(allemails), B))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../resource/lib/public/data_train/index'"
     ]
    }
   ],
   "source": [
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extractfeaturesnaive(path, B):\n",
    "    with open(path, 'r') as femail:\n",
    "        # initialize all-zeros feature vector\n",
    "        v = np.zeros(B)\n",
    "        email = femail.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def loadspamdata(extractfeatures, B=512, path=\"../resource/lib/public/data_train/\"):\n",
    "    '''\n",
    "    INPUT:\n",
    "    extractfeatures : function to extract features\n",
    "    B               : dimensionality of feature space\n",
    "    path            : the path of folder to be processed\n",
    "    \n",
    "    OUTPUT:\n",
    "    X, Y\n",
    "    '''\n",
    "    if path[-1] != '/':\n",
    "        path += '/'\n",
    "    \n",
    "    with open(path + 'index', 'r') as f:\n",
    "        allemails = [x for x in f.read().split('\\n') if ' ' in x]\n",
    "    \n",
    "    xs = np.zeros((len(allemails), B))\n",
    "    ys = np.zeros(len(allemails))\n",
    "    for i, line in enumerate(allemails):\n",
    "        label, filename = line.split(' ')\n",
    "        # make labels +1 for \"spam\" and -1 for \"ham\"\n",
    "        ys[i] = (label == 'spam') * 2 - 1\n",
    "        xs[i, :] = extractfeatures(path + filename, B)\n",
    "    print('Loaded %d input emails.' % len(ys))\n",
    "    return xs, ys\n",
    "\n",
    "X,Y = loadspamdata(extractfeaturesnaive)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your training set. To evaluate your algorithm you should split it off into a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T14:04:34.433096Z",
     "start_time": "2023-06-03T14:04:34.433088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation\n",
    "n, d = X.shape\n",
    "cutoff = int(np.ceil(0.8 * n))\n",
    "# indices of training samples\n",
    "xTr = X[:cutoff,:]\n",
    "yTr = Y[:cutoff]\n",
    "# indices of testing samples\n",
    "xTv = X[cutoff:,:]\n",
    "yTv = Y[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This should generate a training data set <code>xTr</code>, <code>yTr</code> and a validation set <code>xTv</code>, <code>yTv</code> for you. </p>\n",
    "\n",
    "<p>It is now time to implement your classifiers. We will always use the Adagrad gradient descent algorithm, but with various loss functions. \n",
    "First implement the function <code>ridge</code> which computes the ridge regression loss and gradient for a particular data set <code>xTr</code>, <code>yTr</code> and a weight vector <code>w</code>. Make sure you don't forget to incorporate your regularization constant $\\lambda$. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T14:04:34.434103Z",
     "start_time": "2023-06-03T14:04:34.434094Z"
    }
   },
   "outputs": [],
   "source": [
    "w = ar([2,3,4])\n",
    "X = np.array([[1,2,3],[4,5,8],[2,7,4]])\n",
    "y = np.array([1,-1,1])\n",
    "lmbda = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T14:59:42.589595Z",
     "start_time": "2023-06-03T14:59:42.569988Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "boston = load_boston()\n",
    "boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "boston_df.insert(0, 'Price', boston.target)\n",
    "\n",
    "y = boston_df['Price'].values\n",
    "\n",
    "X = boston_df.drop('Price', axis = 1).values\n",
    "\n",
    "d = X.shape[1]\n",
    "\n",
    "w = np.random.rand(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:12:42.599894Z",
     "start_time": "2023-06-03T15:12:42.589964Z"
    }
   },
   "outputs": [],
   "source": [
    "def _ridge_loss(w,xTr,yTr,lmbda):\n",
    "    \n",
    "    n = xTr.shape[0]\n",
    "    d = xTr.shape[1]\n",
    "    u = np.ones(n)\n",
    "    \n",
    "    pred = np.dot(w.T,xTr.T)\n",
    "    error = pred - yTr\n",
    "    \n",
    "    error_square = (error)**2\n",
    "    \n",
    "    first_term = (1/n)*np.dot(error_square,u.T)\n",
    "    \n",
    "    regularizer = lmbda*(np.linalg.norm(w)**2)\n",
    "    # equivalent to \n",
    "    # regularizer = lmbda*(np.dot(w.T,w))\n",
    "    \n",
    "    ridge_loss = first_term + regularizer\n",
    "    \n",
    "    return ridge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:12:42.745558Z",
     "start_time": "2023-06-03T15:12:42.737796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69086.26577850805"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ridge_loss(w,X,y,lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:12:42.899260Z",
     "start_time": "2023-06-03T15:12:42.890542Z"
    }
   },
   "outputs": [],
   "source": [
    "def _ridge_loss_gradient(w,X,y,lmbda):\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    pred = np.dot(w.T,X.T)\n",
    "    error = pred - y\n",
    "    \n",
    "    errorX = (2/n)*np.dot(error,X)\n",
    "    \n",
    "    regularizer_derivative = 2*lmbda*w\n",
    "    \n",
    "    ridge_loss_gradient = errorX + regularizer_derivative\n",
    "    \n",
    "    return ridge_loss_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:12:43.066330Z",
     "start_time": "2023-06-03T15:12:43.056176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.33679738e+03, 5.45550227e+03, 6.18084333e+03, 3.55484965e+01,\n",
       "       2.93320039e+02, 3.21919973e+03, 3.71874794e+04, 1.85828230e+03,\n",
       "       5.54510179e+03, 2.23966785e+05, 9.60549261e+03, 1.84243580e+05,\n",
       "       6.93520224e+03])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ridge_loss_gradient(w,X,y,lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:12:43.220169Z",
     "start_time": "2023-06-03T15:12:43.212127Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge(w,xTr,yTr,lmbda):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    lmbda : regression constant (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    \n",
    "    ridge_loss = _ridge_loss(w,xTr,yTr,lmbda)\n",
    "    \n",
    "    gradient = _ridge_loss_gradient(w,xTr,yTr,lmbda)\n",
    "    \n",
    "    return ridge_loss, gradient\n",
    "    \n",
    "    \n",
    "    \n",
    "#</GRADED>\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>An  alternative to  deriving the gradient analytically is to estimate it numerically. This is very slow, but it is a convenient  way to check your code for correctness.  The following function  uses numerical differentiation to evaluate the correctness of ridge.  If your code is correct, the norm difference between the two should be very small (smaller than $10^{-8}$). \n",
    "Keep in mind that this only checks if the gradient corresponds to the loss, but not if the loss is correct. The function also plots an image of the gradient values (blue) and their estimates (red). If everything is correct, these two should be right on top of each other.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:12:43.529179Z",
     "start_time": "2023-06-03T15:12:43.523301Z"
    }
   },
   "outputs": [],
   "source": [
    "w = w\n",
    "xTr = X\n",
    "yTr = y\n",
    "lmbda = lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:12:43.826009Z",
     "start_time": "2023-06-03T15:12:43.677372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm ratio is 0.0000000000.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numericalgradient(fun,w,e):\n",
    "    # get dimensionality\n",
    "    d = len(w)\n",
    "    # initialize numerical derivative\n",
    "    dh = np.zeros(d)\n",
    "    # go through dimensions\n",
    "    for i in range(d):\n",
    "        # copy the weight vector\n",
    "        nw = w.copy()\n",
    "        # perturb dimension i\n",
    "        nw[i] += e\n",
    "        # compute loss\n",
    "        l1, temp = fun(nw)\n",
    "        # perturb dimension i again\n",
    "        nw[i] -= 2*e\n",
    "        # compute loss\n",
    "        l2, temp = fun(nw)\n",
    "        # the gradient is the slope of the loss\n",
    "        dh[i] = (l1 - l2) / (2*e)\n",
    "    return dh\n",
    "\n",
    "def checkgrad(fun,w,e):\n",
    "    # evaluate symbolic gradient from fun()\n",
    "    loss,dy = fun(w)\n",
    "    # estimate gradient numerically from fun()\n",
    "    dh = numericalgradient(fun,w,e)\n",
    "    \n",
    "    # ii = dy.argsort()\n",
    "    ii = np.array([i for i in range(len(dy))])\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter([i for i in range(len(dy))], dh[ii], c='b', marker='o', s=60)\n",
    "    plt.scatter([i for i in range(len(dy))], dy[ii], c='r', marker='.', s=50)\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Gradient value')\n",
    "    plt.legend([\"numeric\",\"symbolic\"])\n",
    "    \n",
    "    # return the norm of the difference scaled by the norm of the sum\n",
    "    return np.linalg.norm(dh - dy) / np.linalg.norm(dh + dy)\n",
    "\n",
    "# set lmbda (λ) arbitrarily\n",
    "lmbda = 0.1\n",
    "# dimensionality of the input\n",
    "_, d = xTr.shape\n",
    "# evaluate loss on random vector\n",
    "w = np.random.rand(d)\n",
    "# the lambda function notation is an inline way to define a function with only a single argument.\n",
    "ratio = checkgrad(lambda weight: ridge(weight,xTr,yTr,lmbda),w,1e-05)\n",
    "print(\"The norm ratio is %.10f.\" % ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Implement the function <code>adagrad</code> which performs adaptive gradient descent. \n",
    "Make sure to include the tolerance variable to stop early if the norm of the gradient is less than the tolerance value (you can use the function <code>np.linalg.norm(x)</code>). When the norm of the gradient is tiny it means that you have arrived at a minimum.  <br>\n",
    "The first parameter of <code>adagrad</code> is a function which takes a weight vector and returns loss and gradient.\n",
    "</p>                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:15:16.171149Z",
     "start_time": "2023-06-03T15:15:16.147256Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "boston = load_boston()\n",
    "boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "boston_df.insert(0, 'Price', boston.target)\n",
    "\n",
    "y = boston_df['Price'].values\n",
    "\n",
    "X = boston_df.drop('Price', axis = 1).values\n",
    "\n",
    "d = X.shape[1]\n",
    "\n",
    "w = np.random.rand(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:15:16.324982Z",
     "start_time": "2023-06-03T15:15:16.320679Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:15:16.582721Z",
     "start_time": "2023-06-03T15:15:16.571352Z"
    }
   },
   "outputs": [],
   "source": [
    "def _adagrad(func:Callable, \n",
    "             w:np.array, \n",
    "             alpha:float = 0.1, \n",
    "             maxiter:int = 100,\n",
    "             eps:float = 1e-08,\n",
    "             delta:float = 1):\n",
    "    '''\n",
    "    func: function we wish to minimize\n",
    "        This function returns the loss function evaluated at specific point, and the respective loss \n",
    "        function gradient\n",
    "        The adagrad will be executed for each of this outputs, which corresponds to the outputs of each data \n",
    "        point in the dataset\n",
    "        \n",
    "    w: initialized weight vector, which will inside this function, be updated, and according the values of \n",
    "    loss function, given by fun() at each point, will be found the arg_w_min fun() for specific dataset\n",
    "    \n",
    "    alpha: Learning rate, (will it be adaptive for each dimension?)\n",
    "    \n",
    "    eps: Avoids divizion by zero, as it adds to the s term which is used in the np.sqrt(s + eps) denominator to \n",
    "    update the step size alpha_modified for each feature\n",
    "    \n",
    "    delta: value of norm(gradient) allowed to stop the iterations\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    d = w.shape[0]\n",
    "    \n",
    "    s = np.zeros(d)\n",
    "    \n",
    "    # while the norm is bigger than the delta parameter passed, let's continue to go downhill on this loss function\n",
    "    \n",
    "    next_weights = w\n",
    "    losses = []\n",
    "\n",
    "    loss, gradient = func(next_weights)\n",
    "    \n",
    "    while np.linalg.norm(gradient) > delta:\n",
    "        \n",
    "#         print(np.linalg.norm(gradient))\n",
    "#         time.sleep(2)\n",
    "    \n",
    "        s = s + gradient*gradient\n",
    "        \n",
    "        adaptive_term = np.sqrt(s + eps)\n",
    "        \n",
    "        alpha_updated = alpha/adaptive_term\n",
    "    \n",
    "        next_weights = next_weights - alpha_updated*gradient\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        loss, gradient = func(next_weights)\n",
    "        \n",
    "    return next_weights, np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:15:21.469500Z",
     "start_time": "2023-06-03T15:15:17.265160Z"
    }
   },
   "outputs": [],
   "source": [
    "found_w, losses = _adagrad(lambda weight: ridge(weight, xTr, yTr, lmbda), np.random.rand(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:15:21.549856Z",
     "start_time": "2023-06-03T15:15:21.471114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb10813a250>]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:15:21.619261Z",
     "start_time": "2023-06-03T15:15:21.616441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09231356,  0.06841798, -0.02057138,  1.26137508,  0.38259216,\n",
       "        4.51899378,  0.0174796 , -0.81539444,  0.16194917, -0.00970417,\n",
       "       -0.08368691,  0.01727706, -0.53770929])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:16:19.708039Z",
     "start_time": "2023-06-03T15:16:19.541510Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T15:16:34.664715Z",
     "start_time": "2023-06-03T15:16:34.652193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6972354243783133"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_true = y, y_pred = X.dot(found_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    #<GRADED>\n",
    "def adagrad(func,w,alpha,maxiter,eps,delta=1e-02):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    func    : function to minimize\n",
    "              (loss, gradient = func(w))\n",
    "    w       : d dimensional initial weight vector \n",
    "    alpha   : initial gradient descent stepsize (scalar)\n",
    "    maxiter : maximum amount of iterations (scalar)\n",
    "    eps     : epsilon value\n",
    "    delta   : if norm(gradient)<delta, it quits (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "     \n",
    "    w      : d dimensional final weight vector\n",
    "    losses : vector containing loss at each iteration\n",
    "    \"\"\"\n",
    "     \n",
    "    \n",
    "    return w, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, d = xTr.shape\n",
    "eps = 1e-06\n",
    "w, losses = adagrad(lambda weight: ridge(weight, xTr, yTr, lmbda), np.random.rand(d), 1, 1000, eps)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogy(losses, c='r', linestyle='-')\n",
    "plt.xlabel(\"gradient updates\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Adagrad convergence\")\n",
    "print(\"Final loss: %f\" % losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Write the (almost trivial) function <code>linclassify</code> which returns the predictions for a vector <code>w</code> and a data set <code>xTv</code>. (You can take it from a previous project.)</p>\n",
    "\n",
    "<p>After this you can check your training and validation accuracy by running the cell below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def linclassify(w,xTr):\n",
    "    ## fill in your code here\n",
    "    raise NotImplementedError('Your code goes here!')\n",
    "#</GRADED>\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now implement the two other loss functions, <code>logistic</code> and <code>hinge</code>. Start off with <code>logistic</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def logistic(w,xTr,yTr):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    ## fill in your code here\n",
    "    loss = np.sum(np.log(1 + np.exp(-yTr * xTr.dot(w))))\n",
    "#     grad = -(xTr.T * yTr).dot(1 / (1 + np.exp(-yTr * xTr.dot(w)))) + n\n",
    "    grad = -(xTr.T * yTr).dot(1 / (1 + np.exp(yTr * xTr.dot(w))))\n",
    "    return loss, grad\n",
    "#     raise NotImplementedError('Your code goes here!')\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can use the two cells below to test how well this loss function performs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient sanity check\n",
    "_, d = xTr.shape\n",
    "w = np.random.rand(d)\n",
    "ratio = checkgrad(lambda weight: logistic(weight,xTr,yTr),w,1e-05)\n",
    "print(\"The norm ratio is %.10f.\" % ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, losses = adagrad(lambda weight: logistic(weight, xTr, yTr), np.random.rand(d), 1, 1000, 1e-06)\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now implement <code>hinge</code>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def hinge(w,xTr,yTr,lmbda):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    w     : d   dimensional weight vector\n",
    "    xTr   : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr   : n   dimensional vector (each entry is a label)\n",
    "    lmbda : regression constant (scalar)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    loss     : the total loss obtained with w on xTr and yTr (scalar)\n",
    "    gradient : d dimensional gradient at w\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    ## fill in your code here\n",
    "    loss = np.sum(np.maximum((1 - yTr * xTr.dot(w)), np.zeros(n))) + lmbda * w.dot(w)\n",
    "    bools = np.argwhere((1 - yTr * xTr.dot(w)) <= 0)\n",
    "    tmp = yTr.copy()\n",
    "    tmp[bools] = 0\n",
    "    grad = 2 * lmbda * w - xTr.T.dot(tmp)\n",
    "\n",
    "    return loss, grad\n",
    "    \n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You can use the two cells below to test how well this loss function performs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient sanity check\n",
    "lmbda = 0.1\n",
    "_, d = xTr.shape\n",
    "w = np.random.rand(d)\n",
    "ratio = checkgrad(lambda weight: hinge(weight,xTr,yTr,lmbda),w,1e-05)\n",
    "print(\"The norm ratio is %.10f.\" % ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, losses = adagrad(lambda weight: hinge(weight, xTr, yTr, lmbda), np.random.rand(d), 1, 1000, 1e-06)\n",
    "\n",
    "# evaluate training accuracy\n",
    "preds = linclassify(w,xTr)\n",
    "trainingacc = np.mean(preds==yTr)\n",
    "# evaluate testing accuracy\n",
    "preds = linclassify(w,xTv)\n",
    "validationacc = np.mean(preds==yTv)\n",
    "print(\"Training accuracy %2.2f%%\\nValidation accuracy %2.2f%%\\n\" % (trainingacc*100,validationacc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Competition <b>(Optional)</b></h3>\n",
    "\n",
    "<p>The competition for this assignment is split into two components:</p>\n",
    "\n",
    "<ol>\n",
    "<li><b>Feature Extraction</b>:\n",
    "Modify the function <code>extractfeaturescomp</code>.\n",
    "This function takes in a file path <code>path</code> and\n",
    "a feature dimension <code>B</code> and should output a feature vector of dimension <code>B</code>.\n",
    "The autograder will pass in a file path pointing to a file that contains an email,\n",
    "and set <code>B</code> = <code>feature_dimension</code>.\n",
    "We provide <code>extractfeaturesnaive</code> as an example.\n",
    "</li>\n",
    "<li><b>Model Training</b>:\n",
    "Modify the function <code>trainspamfiltercomp</code>.\n",
    "This function takes in training data <code>xTr</code> and training labels <code>yTr</code> and\n",
    "should output a weight vector <code>w</code> for linear classification.\n",
    "We provide an initial implementation using Adagrad and ridge regression.\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "<p>Your model will be trained on the same training set above (loaded by <code>loadspamdata</code>), but we will test its accuracy on a secret dataset of emails.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "feature_dimension = 512\n",
    "def extractfeaturescomp(path, B):\n",
    "    '''\n",
    "    INPUT:\n",
    "    path : file path of email\n",
    "    B    : dimensionality of feature vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    x    : B dimensional vector\n",
    "    '''\n",
    "    x = np.zeros(B)\n",
    "    with open(path, 'r') as femail:\n",
    "        email = femail.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            x[hash(token) % B] = +1\n",
    "    return x\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "def trainspamfiltercomp(xTr, yTr):\n",
    "    '''\n",
    "    INPUT:\n",
    "    xTr : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr : d   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w : d dimensional vector for linear classification\n",
    "    '''\n",
    "#     w = np.random.rand(np.shape(xTr)[1])\n",
    "    _, d = xTr.shape\n",
    "    eps = 1e-06\n",
    "    lmbda = 0.1\n",
    "    w, losses = adagrad(lambda weight: ridge(weight, xTr, yTr, lmbda), np.random.rand(d), 1, 1000, eps)\n",
    "#     raise Exception(w)\n",
    "    return w\n",
    "#</GRADED>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
